import gymnasium as gym
import numpy as np
import time
import matplotlib.pyplot as plt
import pickle
import os
from typing import Tuple, Dict, List

"""
Q-learningç®—æ³•è§£å†³CartPoleå¹³è¡¡é—®é¢˜

æœ¬ç¨‹åºå®ç°äº†ä¸€ä¸ªå®Œæ•´çš„Q-learningå¼ºåŒ–å­¦ä¹ ç³»ç»Ÿï¼Œç”¨äºè®­ç»ƒæ™ºèƒ½ä½“åœ¨CartPoleç¯å¢ƒä¸­ä¿æŒæ†å­å¹³è¡¡ã€‚

CartPoleç¯å¢ƒè¯¦è§£ï¼š
- è§‚å¯Ÿç©ºé—´ï¼š4ç»´è¿ç»­å€¼ [å°è½¦ä½ç½®, å°è½¦é€Ÿåº¦, æ†å­è§’åº¦, æ†å­è§’é€Ÿåº¦]
- åŠ¨ä½œç©ºé—´ï¼š2ä¸ªç¦»æ•£åŠ¨ä½œ [0: å‘å·¦æ¨å°è½¦, 1: å‘å³æ¨å°è½¦]
- æˆåŠŸæ ‡å‡†ï¼šè¿ç»­ä¿æŒæ†å­å¹³è¡¡195æ­¥ä»¥ä¸Š
- å¤±è´¥æ¡ä»¶ï¼šæ†å­å€¾æ–œè¶…è¿‡Â±12Â°æˆ–å°è½¦ç§»å‡ºÂ±2.4å•ä½èŒƒå›´

Q-learningç®—æ³•åŸç†ï¼š
Q-learningæ˜¯ä¸€ç§æ— æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡å­¦ä¹ çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°Q(s,a)æ¥æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚
æ ¸å¿ƒæ›´æ–°å…¬å¼ï¼šQ(s,a) â† Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]
å…¶ä¸­ï¼šÎ±ä¸ºå­¦ä¹ ç‡ï¼ŒÎ³ä¸ºæŠ˜æ‰£å› å­ï¼Œrä¸ºå³æ—¶å¥–åŠ±
"""


class QLearningAgent:
    """
    Q-learningæ™ºèƒ½ä½“ç±»
    
    å°è£…äº†Q-learningç®—æ³•çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬çŠ¶æ€ç¦»æ•£åŒ–ã€Qå€¼ç®¡ç†ã€ç­–ç•¥é€‰æ‹©ç­‰
    """
    
    def __init__(self, learning_rate: float = 0.25, discount_factor: float = 0.99, 
                 initial_exploration_rate: float = 0.4, min_exploration_rate: float = 0.005,
                 exploration_decay: float = 0.9985):
        """
        åˆå§‹åŒ–Q-learningæ™ºèƒ½ä½“
        
        å‚æ•°:
            learning_rate: å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ–°ä¿¡æ¯çš„æ¥å—ç¨‹åº¦ (0-1)
            discount_factor: æŠ˜æ‰£å› å­ï¼Œæ§åˆ¶æœªæ¥å¥–åŠ±çš„é‡è¦æ€§ (0-1)
            initial_exploration_rate: åˆå§‹æ¢ç´¢ç‡
            min_exploration_rate: æœ€å°æ¢ç´¢ç‡
            exploration_decay: æ¢ç´¢ç‡è¡°å‡ç³»æ•°
        """
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.initial_exploration_rate = initial_exploration_rate
        self.min_exploration_rate = min_exploration_rate
        self.exploration_decay = exploration_decay
        
        # Qè¡¨ï¼šå­˜å‚¨çŠ¶æ€-åŠ¨ä½œä»·å€¼å¯¹
        self.q_table: Dict[Tuple, float] = {}
        
        # çŠ¶æ€ç©ºé—´ç¦»æ•£åŒ–å‚æ•°ï¼ˆè¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
        self.cart_position_bins = np.linspace(-2.4, 2.4, 15)    # å°è½¦ä½ç½®ï¼š15ä¸ªåŒºé—´
        self.cart_velocity_bins = np.linspace(-4, 4, 15)         # å°è½¦é€Ÿåº¦ï¼š15ä¸ªåŒºé—´  
        self.pole_angle_bins = np.linspace(-0.2, 0.2, 25)       # æ†å­è§’åº¦ï¼š25ä¸ªåŒºé—´ï¼ˆæœ€å…³é”®ï¼‰
        self.pole_velocity_bins = np.linspace(-4, 4, 20)        # æ†å­è§’é€Ÿåº¦ï¼š20ä¸ªåŒºé—´
    
    def discretize_state(self, observation: np.ndarray) -> Tuple[int, int, int, int]:
        """
        å°†è¿ç»­çš„è§‚å¯Ÿç©ºé—´ç¦»æ•£åŒ–ä¸ºæœ‰é™çš„çŠ¶æ€ç©ºé—´
        
        è¿™æ˜¯Q-learningå¤„ç†è¿ç»­çŠ¶æ€ç©ºé—´çš„å…³é”®æ­¥éª¤ã€‚é€šè¿‡å°†è¿ç»­å€¼æ˜ å°„åˆ°ç¦»æ•£åŒºé—´ï¼Œ
        æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¡¨æ ¼æ–¹æ³•å­˜å‚¨å’Œæ›´æ–°Qå€¼ã€‚
        
        å‚æ•°:
            observation: 4ç»´è§‚å¯Ÿå‘é‡ [cart_position, cart_velocity, pole_angle, pole_velocity]
        
        è¿”å›:
            tuple: ç¦»æ•£åŒ–åçš„çŠ¶æ€å…ƒç»„ï¼Œç”¨ä½œQè¡¨çš„é”®
        """
        cart_position, cart_velocity, pole_angle, pole_velocity = observation
        
        # ä½¿ç”¨np.digitizeå°†è¿ç»­å€¼æ˜ å°„åˆ°ç¦»æ•£åŒºé—´ç´¢å¼•
        # é™åˆ¶ç´¢å¼•èŒƒå›´ï¼Œé¿å…è¶Šç•Œé—®é¢˜
        discretized = [
            min(max(np.digitize(cart_position, self.cart_position_bins), 1), len(self.cart_position_bins)),
            min(max(np.digitize(cart_velocity, self.cart_velocity_bins), 1), len(self.cart_velocity_bins)),
            min(max(np.digitize(pole_angle, self.pole_angle_bins), 1), len(self.pole_angle_bins)),
            min(max(np.digitize(pole_velocity, self.pole_velocity_bins), 1), len(self.pole_velocity_bins))
        ]
        
        return tuple(discretized)
    
    def get_q_value(self, state: Tuple, action: int) -> float:
        """
        è·å–æŒ‡å®šçŠ¶æ€-åŠ¨ä½œå¯¹çš„Qå€¼
        
        å‚æ•°:
            state: ç¦»æ•£åŒ–åçš„çŠ¶æ€
            action: åŠ¨ä½œï¼ˆ0æˆ–1ï¼‰
        
        è¿”å›:
            float: Qå€¼ï¼Œå¦‚æœçŠ¶æ€-åŠ¨ä½œå¯¹ä¸å­˜åœ¨åˆ™è¿”å›0.0ï¼ˆä¹è§‚åˆå§‹åŒ–ï¼‰
        """
        return self.q_table.get((state, action), 0.0)

    def update_q_value(self, state: Tuple, action: int, reward: float, 
                      next_state: Tuple, terminated: bool) -> None:
        """
        ä½¿ç”¨Q-learningç®—æ³•æ›´æ–°Qå€¼
        
        è¿™æ˜¯Q-learningçš„æ ¸å¿ƒæ›´æ–°æ­¥éª¤ï¼Œå®ç°äº†æ—¶åºå·®åˆ†å­¦ä¹ ã€‚
        æ›´æ–°å…¬å¼ï¼šQ(s,a) â† Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]
        
        å‚æ•°:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
            terminated: æ˜¯å¦ä¸ºç»ˆæ­¢çŠ¶æ€
        """
        # ç¡®ä¿å½“å‰çŠ¶æ€-åŠ¨ä½œå¯¹å­˜åœ¨äºQè¡¨ä¸­
        if (state, action) not in self.q_table:
            self.q_table[(state, action)] = 0.0
        
        current_q = self.q_table[(state, action)]
        
        if terminated:
            # ç»ˆæ­¢çŠ¶æ€æ²¡æœ‰æœªæ¥å¥–åŠ±
            target_q = reward
        else:
            # è®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼ï¼ˆè´ªå©ªç­–ç•¥ï¼‰
            next_q_values = [self.get_q_value(next_state, a) for a in range(2)]
            max_next_q = max(next_q_values)
            target_q = reward + self.discount_factor * max_next_q
        
        # æ—¶åºå·®åˆ†æ›´æ–°
        td_error = target_q - current_q
        self.q_table[(state, action)] = current_q + self.learning_rate * td_error

    def choose_action(self, state: Tuple, exploration_rate: float) -> int:
        """
        ä½¿ç”¨Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        å‚æ•°:
            state: å½“å‰çŠ¶æ€
            exploration_rate: å½“å‰æ¢ç´¢ç‡
        
        è¿”å›:
            int: é€‰æ‹©çš„åŠ¨ä½œï¼ˆ0æˆ–1ï¼‰
        """
        if np.random.random() < exploration_rate:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return np.random.randint(2)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            q_values = [self.get_q_value(state, a) for a in range(2)]
            return np.argmax(q_values)
    
    def get_shaped_reward(self, observation: np.ndarray, terminated: bool, step_count: int) -> float:
        """
        å¥–åŠ±å¡‘å½¢å‡½æ•°ï¼šè®¾è®¡æ›´ç²¾ç»†çš„å¥–åŠ±ä¿¡å·æ¥å¼•å¯¼å­¦ä¹ ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        å¥–åŠ±å¡‘å½¢æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„é‡è¦æŠ€æœ¯ï¼Œé€šè¿‡æä¾›é¢å¤–çš„å¥–åŠ±ä¿¡å·æ¥åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚
        
        å‚æ•°:
            observation: å½“å‰è§‚å¯Ÿå€¼
            terminated: æ˜¯å¦ç»ˆæ­¢
            step_count: å½“å‰æ­¥æ•°
        
        è¿”å›:
            float: å¡‘å½¢åçš„å¥–åŠ±å€¼
        """
        if terminated:
            # æ ¹æ®å­˜æ´»æ—¶é—´ç»™äºˆä¸åŒç¨‹åº¦çš„æƒ©ç½šï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            if step_count < 30:
                return -15.0    # ææ—©æœŸå¤±è´¥ï¼šé‡ç½š
            elif step_count < 80:
                return -8.0     # æ—©æœŸå¤±è´¥ï¼šè¾ƒé‡æƒ©ç½š
            elif step_count < 150:
                return -3.0     # ä¸­æœŸå¤±è´¥ï¼šä¸­ç­‰æƒ©ç½š
            else:
                return -0.5     # åæœŸå¤±è´¥ï¼šè½»å¾®æƒ©ç½š
        
        cart_position, cart_velocity, pole_angle, pole_velocity = observation
        
        # åŸºç¡€å­˜æ´»å¥–åŠ±
        reward = 1.0
        
        # ä½ç½®å¥–åŠ±ï¼šé¼“åŠ±å°è½¦ä¿æŒåœ¨ä¸­å¿ƒé™„è¿‘ï¼ˆå¢å¼ºæƒé‡ï¼‰
        position_reward = max(0, 1.0 - abs(cart_position) / 2.4)
        reward += position_reward * 0.15
        
        # è§’åº¦å¥–åŠ±ï¼šé¼“åŠ±æ†å­ä¿æŒå‚ç›´ï¼ˆå¢å¼ºæƒé‡ï¼‰
        angle_reward = max(0, 1.0 - abs(pole_angle) / 0.2)
        reward += angle_reward * 0.3
        
        # ç¨³å®šæ€§å¥–åŠ±ï¼šæƒ©ç½šè¿‡å¤§çš„é€Ÿåº¦ï¼ˆä¼˜åŒ–æƒé‡ï¼‰
        velocity_penalty = (abs(cart_velocity) / 4.0 + abs(pole_velocity) / 4.0)
        reward -= velocity_penalty * 0.08
        
        # é•¿æœŸå­˜æ´»å¥–åŠ±ï¼šé¼“åŠ±æŒç»­å¹³è¡¡ï¼ˆä¼˜åŒ–é˜ˆå€¼ï¼‰
        if step_count > 80:
            reward += 0.3
        if step_count > 150:
            reward += 0.7
        if step_count > 250:
            reward += 1.2
        
        # è¶…çº§ç¨³å®šå¥–åŠ±ï¼šè§’åº¦å’Œä½ç½®éƒ½å¾ˆå¥½æ—¶çš„é¢å¤–å¥–åŠ±
        if abs(pole_angle) < 0.05 and abs(cart_position) < 1.0:
            reward += 0.5
        
        return reward
    
    def get_exploration_rate(self, episode: int) -> float:
        """
        è®¡ç®—å½“å‰å›åˆçš„æ¢ç´¢ç‡
        
        å‚æ•°:
            episode: å½“å‰å›åˆæ•°
        
        è¿”å›:
            float: å½“å‰æ¢ç´¢ç‡
        """
        return max(self.min_exploration_rate, 
                  self.initial_exploration_rate * (self.exploration_decay ** episode))
    
    def get_q_table_size(self) -> int:
        """è·å–Qè¡¨çš„å¤§å°ï¼ˆçŠ¶æ€æ•°é‡ï¼‰"""
        return len(self.q_table)
    
    def save_q_table(self, filepath: str) -> None:
        """
        ä¿å­˜Qè¡¨åˆ°æ–‡ä»¶
        
        å‚æ•°:
            filepath: ä¿å­˜æ–‡ä»¶çš„è·¯å¾„
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(filepath), exist_ok=True)
            
            # ä¿å­˜Qè¡¨å’Œç›¸å…³å‚æ•°
            save_data = {
                'q_table': dict(self.q_table),
                'learning_rate': self.learning_rate,
                'discount_factor': self.discount_factor,
                'initial_exploration_rate': self.initial_exploration_rate,
                'min_exploration_rate': self.min_exploration_rate,
                'exploration_decay': self.exploration_decay,
                'cart_position_bins': self.cart_position_bins,
                'cart_velocity_bins': self.cart_velocity_bins,
                'pole_angle_bins': self.pole_angle_bins,
                'pole_velocity_bins': self.pole_velocity_bins
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(save_data, f)
            
            print(f"âœ… Qè¡¨å·²ä¿å­˜åˆ°: {filepath}")
            print(f"   - Qè¡¨å¤§å°: {len(self.q_table)} ä¸ªçŠ¶æ€")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜Qè¡¨å¤±è´¥: {e}")
    
    def load_q_table(self, filepath: str) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½Qè¡¨
        
        å‚æ•°:
            filepath: Qè¡¨æ–‡ä»¶è·¯å¾„
            
        è¿”å›:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        try:
            if not os.path.exists(filepath):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                return False
            
            with open(filepath, 'rb') as f:
                save_data = pickle.load(f)
            
            # æ¢å¤Qè¡¨å’Œå‚æ•°
            self.q_table = save_data['q_table']
            self.learning_rate = save_data['learning_rate']
            self.discount_factor = save_data['discount_factor']
            self.initial_exploration_rate = save_data['initial_exploration_rate']
            self.min_exploration_rate = save_data['min_exploration_rate']
            self.exploration_decay = save_data['exploration_decay']
            self.cart_position_bins = save_data['cart_position_bins']
            self.cart_velocity_bins = save_data['cart_velocity_bins']
            self.pole_angle_bins = save_data['pole_angle_bins']
            self.pole_velocity_bins = save_data['pole_velocity_bins']
            
            print(f"âœ… Qè¡¨å·²ä»æ–‡ä»¶åŠ è½½: {filepath}")
            print(f"   - Qè¡¨å¤§å°: {len(self.q_table)} ä¸ªçŠ¶æ€")
            print(f"   - å­¦ä¹ ç‡: {self.learning_rate}")
            print(f"   - æŠ˜æ‰£å› å­: {self.discount_factor}")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½Qè¡¨å¤±è´¥: {e}")
            return False
    
    def get_performance_stats(self, episode_rewards: List[float], window_size: int = 100) -> Dict:
        """
        è®¡ç®—è®­ç»ƒæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
        
        å‚æ•°:
            episode_rewards: æ¯å›åˆå¥–åŠ±åˆ—è¡¨
            window_size: æ»‘åŠ¨çª—å£å¤§å°
        
        è¿”å›:
            dict: åŒ…å«å„ç§æ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        if not episode_rewards:
            return {}
        
        stats = {
            'total_episodes': len(episode_rewards),
            'max_reward': max(episode_rewards),
            'min_reward': min(episode_rewards),
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'success_rate': sum(1 for r in episode_rewards if r >= 195) / len(episode_rewards) * 100
        }
        
        # è®¡ç®—æ»‘åŠ¨çª—å£å¹³å‡å€¼
        if len(episode_rewards) >= window_size:
            recent_rewards = episode_rewards[-window_size:]
            stats['recent_mean'] = np.mean(recent_rewards)
            stats['recent_std'] = np.std(recent_rewards)
            stats['recent_success_rate'] = sum(1 for r in recent_rewards if r >= 195) / len(recent_rewards) * 100
        
        # è®¡ç®—å­¦ä¹ è¿›å±•
        if len(episode_rewards) >= 200:
            early_mean = np.mean(episode_rewards[:100])
            late_mean = np.mean(episode_rewards[-100:])
            stats['improvement'] = late_mean - early_mean
        
        return stats

def plot_training_progress(episode_rewards: List[float], agent: QLearningAgent) -> None:
    """
    ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹çš„å¯è§†åŒ–å›¾è¡¨
    
    å‚æ•°:
        episode_rewards: æ¯å›åˆå¥–åŠ±åˆ—è¡¨
        agent: è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    """
    if not episode_rewards:
        print("âš ï¸ æ²¡æœ‰è®­ç»ƒæ•°æ®å¯ä¾›å¯è§†åŒ–")
        return
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # åˆ›å»ºå­å›¾
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Q-learning CartPole è®­ç»ƒè¿‡ç¨‹åˆ†æ', fontsize=16, fontweight='bold')
    
    episodes = range(1, len(episode_rewards) + 1)
    
    # 1. æ¯å›åˆå¥–åŠ±æ›²çº¿
    ax1.plot(episodes, episode_rewards, alpha=0.6, color='lightblue', linewidth=0.8)
    
    # è®¡ç®—æ»‘åŠ¨å¹³å‡
    window_size = min(50, len(episode_rewards) // 10)
    if len(episode_rewards) >= window_size:
        moving_avg = []
        for i in range(len(episode_rewards)):
            start_idx = max(0, i - window_size + 1)
            moving_avg.append(np.mean(episode_rewards[start_idx:i+1]))
        ax1.plot(episodes, moving_avg, color='red', linewidth=2, label=f'æ»‘åŠ¨å¹³å‡({window_size}å›åˆ)')
    
    ax1.axhline(y=195, color='green', linestyle='--', alpha=0.7, label='æˆåŠŸçº¿(195æ­¥)')
    ax1.set_xlabel('å›åˆæ•°')
    ax1.set_ylabel('å¥–åŠ±(æ­¥æ•°)')
    ax1.set_title('è®­ç»ƒè¿‡ç¨‹ - æ¯å›åˆè¡¨ç°')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾
    ax2.hist(episode_rewards, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    ax2.axvline(x=np.mean(episode_rewards), color='red', linestyle='--', 
                label=f'å¹³å‡å€¼: {np.mean(episode_rewards):.1f}')
    ax2.axvline(x=195, color='green', linestyle='--', label='æˆåŠŸçº¿: 195')
    ax2.set_xlabel('å¥–åŠ±(æ­¥æ•°)')
    ax2.set_ylabel('é¢‘æ¬¡')
    ax2.set_title('å¥–åŠ±åˆ†å¸ƒ')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. å­¦ä¹ è¿›å±•ï¼ˆåˆ†æ®µå¹³å‡ï¼‰
    if len(episode_rewards) >= 100:
        segment_size = max(50, len(episode_rewards) // 10)
        segments = []
        segment_means = []
        
        for i in range(0, len(episode_rewards), segment_size):
            end_idx = min(i + segment_size, len(episode_rewards))
            segment_rewards = episode_rewards[i:end_idx]
            segments.append(f'{i+1}-{end_idx}')
            segment_means.append(np.mean(segment_rewards))
        
        ax3.bar(range(len(segments)), segment_means, alpha=0.7, color='lightgreen')
        ax3.axhline(y=195, color='red', linestyle='--', alpha=0.7, label='æˆåŠŸçº¿')
        ax3.set_xlabel('è®­ç»ƒé˜¶æ®µ')
        ax3.set_ylabel('å¹³å‡å¥–åŠ±')
        ax3.set_title('å­¦ä¹ è¿›å±• - åˆ†æ®µå¹³å‡è¡¨ç°')
        ax3.set_xticks(range(len(segments)))
        ax3.set_xticklabels([f'ç¬¬{i+1}æ®µ' for i in range(len(segments))], rotation=45)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
    
    # 4. æ¢ç´¢ç‡è¡°å‡æ›²çº¿
    exploration_rates = [agent.get_exploration_rate(ep) for ep in range(len(episode_rewards))]
    ax4.plot(episodes, exploration_rates, color='orange', linewidth=2)
    ax4.set_xlabel('å›åˆæ•°')
    ax4.set_ylabel('æ¢ç´¢ç‡')
    ax4.set_title('æ¢ç´¢ç‡è¡°å‡')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    stats = agent.get_performance_stats(episode_rewards)
    print(f"\nğŸ“Š è¯¦ç»†è®­ç»ƒç»Ÿè®¡ï¼š")
    print(f"   - æ€»å›åˆæ•°: {stats.get('total_episodes', 0)}")
    print(f"   - å¹³å‡è¡¨ç°: {stats.get('mean_reward', 0):.2f} Â± {stats.get('std_reward', 0):.2f} æ­¥")
    print(f"   - æœ€ä½³è¡¨ç°: {stats.get('max_reward', 0)} æ­¥")
    print(f"   - æœ€å·®è¡¨ç°: {stats.get('min_reward', 0)} æ­¥")
    print(f"   - æ•´ä½“æˆåŠŸç‡: {stats.get('success_rate', 0):.1f}%")
    
    if 'recent_mean' in stats:
        print(f"   - æœ€è¿‘100å›åˆå¹³å‡: {stats['recent_mean']:.2f} Â± {stats['recent_std']:.2f} æ­¥")
        print(f"   - æœ€è¿‘100å›åˆæˆåŠŸç‡: {stats['recent_success_rate']:.1f}%")
    
    if 'improvement' in stats:
        print(f"   - å­¦ä¹ æ”¹è¿›: {stats['improvement']:.2f} æ­¥")


def train_agent(num_episodes: int = 2000) -> Tuple[QLearningAgent, List[float]]:
    """
    è®­ç»ƒQ-learningæ™ºèƒ½ä½“
    
    å‚æ•°:
        num_episodes: è®­ç»ƒå›åˆæ•°
    
    è¿”å›:
        tuple: (è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“, æ¯å›åˆå¥–åŠ±åˆ—è¡¨)
    """
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = gym.make('CartPole-v1')
    agent = QLearningAgent()
    
    print("ğŸš€ å¼€å§‹Q-learningè®­ç»ƒ...")
    print("=" * 80)
    
    episode_rewards = []  # è®°å½•æ¯ä¸ªå›åˆçš„å¥–åŠ±
    best_reward = 0      # è®°å½•æœ€ä½³è¡¨ç°
    consecutive_good_episodes = 0  # è¿ç»­å¥½è¡¨ç°çš„å›åˆæ•°
    
    start_time = time.time()
    
    for episode in range(num_episodes):
        # é‡ç½®ç¯å¢ƒï¼Œè·å–åˆå§‹è§‚å¯Ÿ
        observation, info = env.reset()
        state = agent.discretize_state(observation)
        
        total_reward = 0
        terminated = False
        truncated = False
        step_count = 0
        
        # è·å–å½“å‰æ¢ç´¢ç‡
        exploration_rate = agent.get_exploration_rate(episode)
        
        # å¼€å§‹å›åˆå¾ªç¯
        while not (terminated or truncated):
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.choose_action(state, exploration_rate)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            observation, reward, terminated, truncated, info = env.step(action)
            next_state = agent.discretize_state(observation)
            
            # è·å–å¡‘å½¢å¥–åŠ±
            shaped_reward = agent.get_shaped_reward(observation, terminated or truncated, step_count)
            
            # æ›´æ–°Qå€¼
            agent.update_q_value(state, action, shaped_reward, next_state, terminated or truncated)
            
            # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯
            state = next_state
            total_reward += reward  # ä½¿ç”¨åŸå§‹å¥–åŠ±è®°å½•è¡¨ç°
            step_count += 1
        
        episode_rewards.append(total_reward)
        
        # è·Ÿè¸ªè¡¨ç°
        if total_reward > best_reward:
            best_reward = total_reward
            consecutive_good_episodes = 0
        elif total_reward >= 195:  # CartPoleæˆåŠŸæ ‡å‡†
            consecutive_good_episodes += 1
        else:
            consecutive_good_episodes = 0
        
        # å®šæœŸæ‰“å°è¿›åº¦
        if episode % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:]) if episode_rewards else 0
            elapsed_time = time.time() - start_time
            print(f"å›åˆ {episode:4d} | å¥–åŠ±: {total_reward:3.0f} | å¹³å‡å¥–åŠ±: {avg_reward:6.2f} | "
                  f"æ¢ç´¢ç‡: {exploration_rate:.3f} | æœ€ä½³: {best_reward} | Qè¡¨å¤§å°: {agent.get_q_table_size()} | "
                  f"æ—¶é—´: {elapsed_time:.1f}s")
        
        # æ—©åœæœºåˆ¶
        if consecutive_good_episodes >= 100:
            print(f"\nğŸ‰ æ™ºèƒ½ä½“å·²å­¦ä¼šå¹³è¡¡ï¼ç¬¬ {episode} å›åˆè¾¾åˆ°ç¨³å®šè¡¨ç°")
            break
    
    env.close()
    
    total_time = time.time() - start_time
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡ï¼š")
    print(f"   - æ€»å›åˆæ•°: {len(episode_rewards)}")
    print(f"   - Qè¡¨å¤§å°: {agent.get_q_table_size()} ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹")
    print(f"   - æœ€ä½³è¡¨ç°: {best_reward} æ­¥")
    print(f"   - å¹³å‡è¡¨ç°: {np.mean(episode_rewards[-100:]):.2f} æ­¥ï¼ˆæœ€å100å›åˆï¼‰")
    print(f"   - è®­ç»ƒæ—¶é—´: {total_time:.2f} ç§’")
    print("=" * 80)
    
    return agent, episode_rewards

def test_agent(agent: QLearningAgent, render: bool = True, num_tests: int = 5) -> List[float]:
    """
    æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    
    å‚æ•°:
        agent: è®­ç»ƒå¥½çš„Q-learningæ™ºèƒ½ä½“
        render: æ˜¯å¦æ˜¾ç¤ºå›¾å½¢ç•Œé¢
        num_tests: æµ‹è¯•å›åˆæ•°
    
    è¿”å›:
        list: æ¯æ¬¡æµ‹è¯•çš„å¥–åŠ±åˆ—è¡¨
    """
    print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•æ™ºèƒ½ä½“ï¼ˆ{num_tests}å›åˆï¼‰...")
    print("=" * 60)
    
    test_rewards = []
    
    for test_episode in range(num_tests):
        # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
        if render and test_episode == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡æµ‹è¯•æ—¶æ˜¾ç¤ºå›¾å½¢
            test_env = gym.make('CartPole-v1', render_mode='human')
            print("ğŸ® å›¾å½¢ç•Œé¢å·²å¼€å¯ï¼Œè§‚å¯Ÿæ™ºèƒ½ä½“è¡¨ç°...")
        else:
            test_env = gym.make('CartPole-v1')
        
        observation, info = test_env.reset()
        state = agent.discretize_state(observation)
        
        terminated = False
        truncated = False
        total_reward = 0
        step_count = 0
        
        while not (terminated or truncated):
            # çº¯åˆ©ç”¨ç­–ç•¥ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            q_values = [agent.get_q_value(state, a) for a in range(2)]
            action = np.argmax(q_values)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            observation, reward, terminated, truncated, info = test_env.step(action)
            state = agent.discretize_state(observation)
            
            total_reward += reward
            step_count += 1
            
            # åœ¨å›¾å½¢æ¨¡å¼ä¸‹æ˜¾ç¤ºè¿›åº¦
            if render and test_episode == 0 and step_count % 50 == 0:
                print(f"   æ­¥æ•°: {step_count}, å½“å‰å¥–åŠ±: {total_reward}")
        
        test_rewards.append(total_reward)
        
        # åˆ¤æ–­ç»“æŸåŸå› 
        if terminated:
            end_reason = "æ†å­å€’ä¸‹æˆ–å°è½¦è¶…å‡ºè¾¹ç•Œ"
        elif truncated:
            end_reason = "è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶(500æ­¥)"
        else:
            end_reason = "æœªçŸ¥åŸå› "
        
        print(f"æµ‹è¯• {test_episode + 1}: {total_reward:3.0f} æ­¥ - {end_reason}")
        
        test_env.close()
        
        # åœ¨å›¾å½¢æµ‹è¯•åç¨ä½œåœé¡¿
        if render and test_episode == 0:
            time.sleep(1)
    
    # æµ‹è¯•ç»“æœç»Ÿè®¡
    avg_reward = np.mean(test_rewards)
    max_reward = max(test_rewards)
    min_reward = min(test_rewards)
    success_rate = sum(1 for r in test_rewards if r >= 195) / len(test_rewards) * 100
    
    print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœç»Ÿè®¡ï¼š")
    print(f"   - å¹³å‡è¡¨ç°: {avg_reward:.2f} æ­¥")
    print(f"   - æœ€ä½³è¡¨ç°: {max_reward} æ­¥")
    print(f"   - æœ€å·®è¡¨ç°: {min_reward} æ­¥")
    print(f"   - æˆåŠŸç‡: {success_rate:.1f}% (â‰¥195æ­¥)")
    print("=" * 60)
    
    return test_rewards


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºQ-learningç®—æ³•åœ¨CartPoleç¯å¢ƒä¸­çš„åº”ç”¨"""
    print("ğŸ¯ CartPole Q-learning æ™ºèƒ½ä½“è®­ç»ƒä¸æµ‹è¯•")
    print("=" * 80)
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„Qè¡¨
    q_table_path = "models/cartpole_q_table.pkl"
    load_existing = False
    
    if os.path.exists(q_table_path):
        user_input = input(f"å‘ç°å·²ä¿å­˜çš„Qè¡¨æ–‡ä»¶: {q_table_path}\næ˜¯å¦åŠ è½½å·²æœ‰æ¨¡å‹ï¼Ÿ(y/n): ").lower().strip()
        load_existing = user_input in ['y', 'yes', 'æ˜¯']
    
    if load_existing:
        # åŠ è½½å·²æœ‰æ¨¡å‹
        agent = QLearningAgent()
        if agent.load_q_table(q_table_path):
            print("ğŸ”„ ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
            test_rewards = test_agent(agent, render=True, num_tests=5)
        else:
            print("âŒ åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°è®­ç»ƒ...")
            load_existing = False
    
    if not load_existing:
        # è®­ç»ƒæ–°æ¨¡å‹
        agent, training_rewards = train_agent(num_episodes=2000)
        
        # æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
        print(f"\nğŸ“ˆ ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å›¾è¡¨...")
        plot_training_progress(training_rewards, agent)
        
        # ä¿å­˜è®­ç»ƒå¥½çš„Qè¡¨
        print(f"\nğŸ’¾ ä¿å­˜è®­ç»ƒå¥½çš„Qè¡¨...")
        agent.save_q_table(q_table_path)
        
        # æµ‹è¯•æ™ºèƒ½ä½“
        test_rewards = test_agent(agent, render=True, num_tests=5)
        
        # æ˜¾ç¤ºå­¦ä¹ æ›²çº¿ä¿¡æ¯
        stats = agent.get_performance_stats(training_rewards)
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ€»å›åˆæ•°: {stats['total_episodes']}")
        print(f"   - å¹³å‡å¥–åŠ±: {stats['mean_reward']:.2f}")
        print(f"   - æœ€å¤§å¥–åŠ±: {stats['max_reward']:.2f}")
        print(f"   - æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        print(f"   - Qè¡¨å¤§å°: {agent.get_q_table_size()} ä¸ªçŠ¶æ€")
        
        if 'improvement' in stats:
            print(f"   - å­¦ä¹ æ”¹è¿›: {stats['improvement']:.2f} æ­¥")
    
    print(f"\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()